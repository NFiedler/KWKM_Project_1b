{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngram_finder import NGramFinder\n",
    "import csv\n",
    "import nltk\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change filepath and comment column index below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now looking at../data/2020-11-20 Jungfernstieg_Kommentare.csv\n",
      "now looking at../data/2020-11-20 Jungfernstieg_Beiträge.csv\n",
      "done with calculating base values\n",
      "created dict for main case\n",
      "Unique:  ('u.a.', 'gewerbliche')\n",
      "Unique:  ('strom-', 'wasseranschlüssen?!')\n",
      "Unique:  ('bootsanleger', 'etc.')\n",
      "Unique:  ('zwecke', 'strom-')\n",
      "Unique:  ('geben,', 'u.a.')\n",
      "Unique:  ('gewerbliche', 'zwecke')\n",
      "Unique:  ('etc.', 'geben,')\n",
      "Unique:  ('billhorner', 'brückenstraße')\n",
      "Unique:  ('brandshofer', 'deich')\n",
      "Unique:  ('endlich', 'mal')\n",
      "Unique:  ('ja', 'schon')\n",
      "Unique:  ('neben', 'elbtower')\n",
      "Unique:  ('city', 'nord')\n",
      "Unique:  ('ja', 'eh')\n",
      "Unique:  ('erhalten', 'bleiben')\n",
      "Unique:  ('variante', 'a')\n",
      "Unique:  ('a', 'variante')\n",
      "Higher Value: ('immer', 'mehr')\n",
      "Unique:  ('neue', 'moschee')\n",
      "Unique:  ('entwurf', 'b')\n"
     ]
    }
   ],
   "source": [
    "# this is the file for which we are interested in the ngrams\n",
    "main_file = \"../data/2020-11-20 Stadteingang Elbbrücken_Kommentare.csv\"\n",
    "csvfile = glob.glob(f\"../data/*.csv\")\n",
    "\n",
    "ngrams = []\n",
    "count_of_all_words = 0\n",
    "\n",
    "for file in csvfile:\n",
    "    # exclude file that we want to analyze\n",
    "    # otherwise if something comes up very often in this case,\n",
    "    # it might still seem popular even if it's only popular here\n",
    "    if file == main_file:\n",
    "        continue\n",
    "    if 'Beiträge'in file:\n",
    "        comment_index = 10\n",
    "    if 'Kommentare' in file:\n",
    "        comment_index = 3\n",
    "\n",
    "    print(\"now looking at\" + str(file))\n",
    "    curr_ngrams, count = Gen_Freq_Ngrams(file, comment_index)\n",
    "    ngrams.append(curr_ngrams)\n",
    "    count_of_all_words += count\n",
    "    \n",
    "# generate absolute amount of ngrams coming up\n",
    "ngramdict = {}\n",
    "for file in ngrams:\n",
    "    for ngram in file:\n",
    "        current = ngramdict.get(ngram[0])\n",
    "        if current == None:\n",
    "            ngramdict[ngram[0]] = ngram[1]\n",
    "        else:\n",
    "            ngramdict[ngram[0]] += ngram[1]\n",
    "# print(ngramdict)\n",
    "\n",
    "# generate relative amount of ngrams\n",
    "for key in ngramdict.keys():\n",
    "    ngramdict[key] = ngramdict[key] / count_of_all_words\n",
    "\n",
    "# sort keys, this allows to have a look at what are the most popular ngrams\n",
    "sorted_keys = sorted(ngramdict, key=ngramdict.get)\n",
    "sorted_dict = {}\n",
    "for w in sorted_keys:\n",
    "    sorted_dict[w] = ngramdict[w]\n",
    "# print(sorted_dict)\n",
    "\n",
    "# look at just the maximum value\n",
    "max_value_ngram = max(sorted_dict, key = sorted_dict.get)\n",
    "\n",
    "print(\"done with calculating base values\")\n",
    "main_ngrams, count = Gen_Freq_Ngrams(main_file, 3)\n",
    "main_ngrams_dict = {}\n",
    "for ngram in main_ngrams:\n",
    "    main_ngrams_dict[ngram[0]] = ngram[1] / count\n",
    "print(\"created dict for main case\")\n",
    "for key in main_ngrams_dict.keys():\n",
    "    overallRating = ngramdict.get(key)\n",
    "    #print(f\"key: {key}, overall rating {overallRating}, main rating {main_ngrams_dict[key]}\")\n",
    "    if overallRating == None:\n",
    "        # this ngram didn't show up in any of the other cases, so it is likely relevant\n",
    "        if main_ngrams_dict[key] > 0.00025:\n",
    "            print(f\"Unique:  {key}\")\n",
    "    elif overallRating * 0.1 < main_ngrams_dict[key]:\n",
    "        # this ngram was more popular here than in other cases, so it is likely relevant\n",
    "        print(f\"Higher Value: {key}\")\n",
    "    \n",
    "#print(main_ngrams_dict)    \n",
    "# filepath = '../data/2020-11-20 Stadteingang Elbbruecken_Kommentare.csv'\n",
    "\n",
    "\n",
    "\n",
    "# for the elbbruecken file the comments are in the fourth column\n",
    "#comment_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gen_Freq_Ngrams(filepath, comment_index):\n",
    "\n",
    "    ngf = NGramFinder()\n",
    "    with open(filepath, 'r', encoding='utf8') as file:\n",
    "        lines = list(csv.reader(file, delimiter=';'))\n",
    "\n",
    "        comments = [line[comment_index] for line in lines]\n",
    "        \n",
    "        count_all_words = 0\n",
    "        for comment in comments:\n",
    "            count_all_words += len(comment.split(' '))\n",
    "        count = ngf.get_ngram_count(comments, 2, [])\n",
    "        # print(\"Höchste Häufigkeit eines N-Grams: \" + str(max(count.values())))\n",
    "        frequent_n_grams = list()\n",
    "        # length_all_comments = \n",
    "        for text, count in count.items():\n",
    "            if count >= 3:\n",
    "                frequent_n_grams.append((text, count))\n",
    "                #print(text)\n",
    "    return frequent_n_grams, count_all_words           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_count_before = 5 #float('inf')\n",
    "word_count_after = 3\n",
    "\n",
    "for ngram in frequent_n_grams:\n",
    "    print (\"Suche nach \" + str(ngram) + \"\\n\")\n",
    "    for context in ngf.get_contexts_for_ngram(comments, ngram, word_count_before, word_count_after):\n",
    "        print(\"    \" + context + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
